{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d10d20-842a-4b2b-a957-3bddb3a154a1",
   "metadata": {},
   "source": [
    "# Modelo alométrico para inferencia del peso $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46ea80-b005-4d43-baa8-1b6b5589ea96",
   "metadata": {},
   "source": [
    "A partir de los estudios matematicos realizados en los capítulos anteriores podemos desarrollar un *pipeline* que dados los valores de longitud $L$ y anchura $A$ de un lenguado obtenidos por medición directa o mediante visión artificial obtengamos el peso estimado. La trasnformada logaritmica produce una normalización de la distribución sesgada del peso y una linealización de las relaciones exponenciales de Peso vs. Longitud y Peso vs. Anchura, lo que nos permite usar el método `LinearRegression` de la librería `scikit-learn`. Esta librería ofrece implementaciones listas para usar de los algoritmos más frecuentes en clasificación, regresión, clustering, reducción de dimensión, validación de modelos y preprocesado, todo ello bajo una API unificada que se resume en los métodos fit, predict y transform. Esta coherencia facilita encadenar pasos mediante *pipelines*, realizar búsquedas de hiperparámetros y/o estimar el error de generalización.\n",
    "\n",
    "En aprendizaje automático, el **error de generalización** (también denominado *riesgo esperado*) es la discrepancia estadística entre las predicciones de un modelo y los valores reales que se observarían si el modelo se aplicase a todo el universo de datos posibles, no sólo a la muestra con la que fue entrenado. Este error uede estimarse empíricamente y, a la vez, reducirse mediante validación cruzada K-Fold. Esta técnica particiona aleatoriamenteel conjunto de datos en $K$ pliegues de tamaño similar, cada iteración reserva un pliegue como “muestra externa” y entrena el modelo con los $K – 1$ restantes, de modo que todas las observaciones se emplean sucesivamente como datos no vistos; el promedio de la métrica sobre los $K$ ciclos proporciona una estimación casi insesgada del riesgo esperado y presenta menor varianza que un único *hold-out*. Además, esa estructura repetida permite comparar hiperparámetros, arquitecturas o transformaciones y seleccionar la configuración que minimiza la pérdida promedio en los pliegues, lo cual actúa como un proceso de regularización implícita que atenúa el sobreajuste y, por ende, disminuye el error de generalización del modelo final entrenado con el conjunto completo.\n",
    "\n",
    "`````{admonition} ¿Que és *hold-out?*\n",
    ":class: tip\n",
    "En aprendizaje automático, un *hold-out* es una estrategia de validación que consiste en apartar de forma permanente una fracción del dataset —típicamente entre el 20 % y el 30 %— para usarla exclusivamente como conjunto de prueba (test set). El resto de las observaciones se emplea para entrenar el modelo (y opcionalmente para ajustar hiperparámetros mediante un subconjunto de validación interno). Durante el entrenamiento el modelo nunca “ve” los datos del *hold-out*; así, el rendimiento calculado sobre ese bloque reservado ofrece una estimación objetiva de su capacidad de generalizar a datos completamente nuevos. El método es sencillo y eficiente cuando se dispone de un volumen de datos muy grande, pero puede ser inestable en muestras moderadas o pequeñas, ya que la estimación depende fuertemente de cómo se haya realizado la partición; por esa razón suele sustituirse o complementarse con técnicas más robustas como la validación cruzada K-Fold.\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa96ae92-86c4-45ed-9bb5-1c11a4752e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo alométrico:  W = 0.03904 · L^1.895 · A^1.009\n",
      "R² (entrenamiento): 0.9722\n",
      "R² (5-fold CV):     0.9654\n",
      "RMSE log-espacio (CV): 0.1109\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.972\n",
      "Model:                            OLS   Adj. R-squared:                  0.972\n",
      "Method:                 Least Squares   F-statistic:                     3568.\n",
      "Date:                Wed, 11 Jun 2025   Prob (F-statistic):          1.93e-159\n",
      "Time:                        14:08:43   Log-Likelihood:                 177.68\n",
      "No. Observations:                 207   AIC:                            -349.4\n",
      "Df Residuals:                     204   BIC:                            -339.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2431      0.123    -26.447      0.000      -3.485      -3.001\n",
      "lnL            1.8952      0.103     18.477      0.000       1.693       2.097\n",
      "lnA            1.0092      0.088     11.427      0.000       0.835       1.183\n",
      "==============================================================================\n",
      "Omnibus:                       36.827   Durbin-Watson:                   1.883\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              140.764\n",
      "Skew:                          -0.618   Prob(JB):                     2.71e-31\n",
      "Kurtosis:                       6.846   Cond. No.                         60.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 1. CARGA DE LIBRERÍAS\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import statsmodels.api as sm            # para diagnóstico opcional\n",
    "\n",
    "# ==============================\n",
    "# 2. LECTURA DEL DATASET + LIMPIEZA\n",
    "# ==============================\n",
    "df = pd.read_excel('./data/Dimensiones_lenguado.xlsx')\n",
    "\n",
    "# Eliminamos los outliers detectados con IQR, Z-Score e Isolation Forest\n",
    "df = df.drop(index=[0, 1]).reset_index(drop=True)\n",
    "\n",
    "# Renombramos columnas a algo más manejable\n",
    "df = df.rename(columns={\n",
    "    'Peso (g)': 'W',\n",
    "    'Longitud (cm)': 'L',\n",
    "    'Anchura (cm)': 'A'\n",
    "})\n",
    "\n",
    "# ==============================\n",
    "# 3. TRANSFORMACIÓN LOGARÍTMICA\n",
    "#    ln(W) = ln(k) + a·ln(L) + b·ln(A)\n",
    "# ==============================\n",
    "df['lnW'] = np.log(df['W'])\n",
    "df['lnL'] = np.log(df['L'])\n",
    "df['lnA'] = np.log(df['A'])\n",
    "\n",
    "X = df[['lnL', 'lnA']].values          # variables predictoras\n",
    "y = df['lnW'].values                   # variable respuesta\n",
    "\n",
    "# ==============================\n",
    "# 4. AJUSTE DEL MODELO\n",
    "# ==============================\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "a, b       = linreg.coef_              # exponentes\n",
    "ln_k       = linreg.intercept_         # ln(k)\n",
    "k          = np.exp(ln_k)              # constante de proporcionalidad\n",
    "r2_train   = linreg.score(X, y)\n",
    "\n",
    "# ==============================\n",
    "# 5. VALIDACIÓN CRUZADA (5-fold)\n",
    "# ==============================\n",
    "kf      = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_cv   = cross_val_score(linreg, X, y, cv=kf, scoring='r2').mean()\n",
    "rmse_cv = -cross_val_score(\n",
    "    linreg, X, y,\n",
    "    cv=kf,\n",
    "    scoring=make_scorer(\n",
    "        lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    )\n",
    ").mean()\n",
    "\n",
    "# ==============================\n",
    "# 6. RESULTADOS\n",
    "# ==============================\n",
    "print(f\"Modelo alométrico:  W = {k:.5f} · L^{a:.3f} · A^{b:.3f}\")\n",
    "print(f\"R² (entrenamiento): {r2_train:.4f}\")\n",
    "print(f\"R² (5-fold CV):     {r2_cv:.4f}\")\n",
    "print(f\"RMSE log-espacio (CV): {rmse_cv:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 7. DIAGNÓSTICO OPCIONAL (Statsmodels)\n",
    "# ==============================\n",
    "X_sm = sm.add_constant(df[['lnL', 'lnA']])\n",
    "ols   = sm.OLS(y, X_sm).fit()\n",
    "print(ols.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15947968-3551-4381-a4c9-4d49b1d4a635",
   "metadata": {},
   "source": [
    "## Conclusión\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93adec-2769-45c1-996d-0a9a403c3c81",
   "metadata": {},
   "source": [
    "El modelo obtenido ajusta un 97 % de la variabilidad del peso, con coeficientes altamente significativos y un ajuste global validado por un F-statistic muy elevado. El diagnóstico de residuos revela un ajuste cuantitativamente sólido pero con ligeras desviaciones respecto a los supuestos clásicos: el estadístico Durbin–Watson ≈ 1.88 descarta autocorrelación, mientras que el número de condición (Cond. Nº) 60.2 indica colinealidad moderada entre ln L y ln A que no compromete la predicción pero puede inflar la varianza de los coeficientes. Sin embargo, los tests Omnibus y Jarque–Bera (p ≈ 0) señalan que los residuos no son estrictamente normales. En conjunto, el modelo explica el 97 % de la variación y no muestra problemas graves de dependencia o inestabilidad, de modo que resulta fiable para estimar el peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34d790-415a-4917-8fec-217599584062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
